2025-12-12 17:11:16,032 - INFO - Quantized model created and moved to device.
2025-12-12 17:11:16,040 - INFO - model structure:
LatentDiffusion(
  (model): Unet(
    (init_conv_mask): SwinTransformer(
      (first_coonv): MySequential(
        (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (1): Permute()
        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (features): ModuleList(
        (0): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PatchMerging(
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): PatchMerging(
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): PatchMerging(
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        )
        (6): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.5, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )
    (init_conv): MySequential(
      (0): (QuantConv2d_ar(
        131, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (1): GroupNorm(8, 128, eps=1e-05, affine=True)
    )
    (projects): ModuleList(
      (0): (QuantConv2d_ar(
        128, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (1): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (2): (QuantConv2d_ar(
        512, 256, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (3): (QuantConv2d_ar(
        1024, 512, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
    )
    (time_mlp): MySequential(
      (0): GaussianFourierProjection()
      (1): (QuantLinear_diff(
        in_features=128, out_features=512, bias=True
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      )input_quant=False, weight_quant=False)
      (2): GELU(approximate=none)
      (3): (QuantLinear_diff(
        in_features=512, out_features=512, bias=True
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      )input_quant=False, weight_quant=False)
    )
    (downs): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
    )
    (downs_mask): ModuleList()
    (ups): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
    )
    (relation_layers_down): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
    )
    (relation_layers_up): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
    )
    (ups2): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=False, weight_quant=False)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
      )
    )
    (relation_layers_up2): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=False, weight_quant=False)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=False, weight_quant=False)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=False, weight_quant=False)
          )
        )
      )
    )
    (mid_block1): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=1024, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=False, weight_quant=False)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): Identity()
    )
    (mid_attn): Residual(
      (fn): PreNorm(
        (fn): Attention(
          (to_qkv): (QuantConv2d_ar(
            512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
          (to_out): (QuantConv2d_ar(
            128, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=False, weight_quant=False)
        )
        (norm): LayerNorm()
      )
    )
    (mid_block2): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=1024, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=False, weight_quant=False)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): Identity()
    )
    (decouple1): MySequential(
      (0): GroupNorm(8, 512, eps=1e-05, affine=True)
      (1): (QuantConv2d_ar(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (2): BlockFFT()
    )
    (decouple2): MySequential(
      (0): GroupNorm(8, 512, eps=1e-05, affine=True)
      (1): (QuantConv2d_ar(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
      (2): BlockFFT()
    )
    (final_res_block): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=256, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=False, weight_quant=False)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
    )
    (final_conv): (QuantConv2d_ar(
      128, 3, kernel_size=(1, 1), stride=(1, 1)
      (input_quantizer): UniformQuantizer_diff()
      (weight_quantizer): UniformQuantizer_diff()
    ), input_quant=False, weight_quant=False)
    (final_res_block2): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=256, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=False, weight_quant=False)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=False, weight_quant=False)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=False, weight_quant=False)
    )
    (final_conv2): (QuantConv2d_ar(
      128, 3, kernel_size=(1, 1), stride=(1, 1)
      (input_quantizer): UniformQuantizer_diff()
      (weight_quantizer): UniformQuantizer_diff()
    ), input_quant=False, weight_quant=False)
  )
  (first_stage_model): AutoencoderKL(
    (encoder): Encoder(
      (conv_in): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (decoder): Decoder(
      (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (up): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
      (conv_out): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_logvar): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): LPIPSWithDiscriminator(
      (perceptual_loss): LPIPS(
        (scaling_layer): ScalingLayer()
        (net): vgg16(
          (slice1): Sequential(
            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (3): ReLU(inplace=True)
          )
          (slice2): Sequential(
            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (6): ReLU(inplace=True)
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (8): ReLU(inplace=True)
          )
          (slice3): Sequential(
            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (11): ReLU(inplace=True)
            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (13): ReLU(inplace=True)
            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (15): ReLU(inplace=True)
          )
          (slice4): Sequential(
            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (18): ReLU(inplace=True)
            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (20): ReLU(inplace=True)
            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (22): ReLU(inplace=True)
          )
          (slice5): Sequential(
            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (25): ReLU(inplace=True)
            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (27): ReLU(inplace=True)
            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (29): ReLU(inplace=True)
          )
        )
        (lin0): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin1): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin2): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin3): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin4): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
      (discriminator): NLayerDiscriminator(
        (main): Sequential(
          (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.2, inplace=True)
          (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): LeakyReLU(negative_slope=0.2, inplace=True)
          (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (7): LeakyReLU(negative_slope=0.2, inplace=True)
          (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
          (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (10): LeakyReLU(negative_slope=0.2, inplace=True)
          (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (quant_conv): Conv2d(6, 6, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
2025-12-12 17:11:16,043 - INFO - Setting quantization state: input_quant=True, weight_quant=True
2025-12-12 17:11:16,050 - INFO - model structure:
LatentDiffusion(
  (model): Unet(
    (init_conv_mask): SwinTransformer(
      (first_coonv): MySequential(
        (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
        (1): Permute()
        (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (features): ModuleList(
        (0): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.0, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (proj): Linear(in_features=128, out_features=128, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)
            (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=128, out_features=512, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=512, out_features=128, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (1): PatchMerging(
          (reduction): Linear(in_features=512, out_features=256, bias=False)
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
        (2): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (proj): Linear(in_features=256, out_features=256, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)
            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=256, out_features=1024, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=1024, out_features=256, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (3): PatchMerging(
          (reduction): Linear(in_features=1024, out_features=512, bias=False)
          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
        (4): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (6): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (7): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (8): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (9): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (10): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (11): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (12): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (13): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (14): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (15): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (16): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (17): SwinTransformerBlock(
            (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (proj): Linear(in_features=512, out_features=512, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)
            (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=512, out_features=2048, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=2048, out_features=512, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
        (5): PatchMerging(
          (reduction): Linear(in_features=2048, out_features=1024, bias=False)
          (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
        )
        (6): MySequential(
          (0): SwinTransformerBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (attn): ShiftedWindowAttention(
              (qkv): Linear(in_features=1024, out_features=3072, bias=True)
              (proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (stochastic_depth): StochasticDepth(p=0.5, mode=row)
            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (mlp): MLP(
              (0): Linear(in_features=1024, out_features=4096, bias=True)
              (1): GELU(approximate=none)
              (2): Dropout(p=0.0, inplace=False)
              (3): Linear(in_features=4096, out_features=1024, bias=True)
              (4): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (avgpool): AdaptiveAvgPool2d(output_size=1)
      (head): Linear(in_features=1024, out_features=1000, bias=True)
    )
    (init_conv): MySequential(
      (0): (QuantConv2d_ar(
        131, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (1): GroupNorm(8, 128, eps=1e-05, affine=True)
    )
    (projects): ModuleList(
      (0): (QuantConv2d_ar(
        128, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (1): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (2): (QuantConv2d_ar(
        512, 256, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (3): (QuantConv2d_ar(
        1024, 512, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
    )
    (time_mlp): MySequential(
      (0): GaussianFourierProjection()
      (1): (QuantLinear_diff(
        in_features=128, out_features=512, bias=True
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      )input_quant=True, weight_quant=True)
      (2): GELU(approximate=none)
      (3): (QuantLinear_diff(
        in_features=512, out_features=512, bias=True
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      )input_quant=True, weight_quant=True)
    )
    (downs): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): Identity()
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
    )
    (downs_mask): ModuleList()
    (ups): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
    )
    (relation_layers_down): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
    )
    (relation_layers_up): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
    )
    (ups2): ModuleList(
      (0): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            1024, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (1): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=1024, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              768, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            768, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (2): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 256, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            384, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                256, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 256, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): MySequential(
          (0): Upsample(scale_factor=2.0, mode=nearest)
          (1): (QuantConv2d_ar(
            256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
      )
      (3): ModuleList(
        (0): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (1): ResnetBlock(
          (mlp): MySequential(
            (0): SiLU()
            (1): (QuantLinear_diff(
              in_features=512, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
          )
          (block1): Block(
            (proj): (QuantConv2d_ar(
              256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (block2): Block(
            (proj): (QuantConv2d_ar(
              128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
            (act): SiLU()
          )
          (res_conv): (QuantConv2d_ar(
            256, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (2): Residual(
          (fn): PreNorm(
            (fn): LinearAttention(
              (to_qkv): (QuantConv2d_ar(
                128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (to_out): MySequential(
                (0): (QuantConv2d_ar(
                  128, 128, kernel_size=(1, 1), stride=(1, 1)
                  (input_quantizer): UniformQuantizer_diff()
                  (weight_quantizer): UniformQuantizer_diff()
                ), input_quant=True, weight_quant=True)
                (1): LayerNorm()
              )
            )
            (norm): LayerNorm()
          )
        )
        (3): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
      )
    )
    (relation_layers_up2): ModuleList(
      (0): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[1, 1], stride=[1, 1], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (1): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            256, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            512, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=512, out_features=512, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                512, 1024, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                1024, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              1024, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 512, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              512, 512, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (2): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            256, 256, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[4, 4], stride=[4, 4], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=256, out_features=256, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                256, 512, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                512, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              512, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 256, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              256, 256, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
      (3): RelationNet(
        (input_conv1): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (input_conv2): MySequential(
          (0): (QuantConv2d_ar(
            128, 128, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)
        )
        (attentions): ModuleList(
          (0): BasicAttetnionLayer(
            (avgpool_q): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (avgpool_k): AvgPool2d(kernel_size=[8, 8], stride=[8, 8], padding=0)
            (softmax): Softmax(dim=-1)
            (q_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (k_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (v_lin): (QuantLinear_diff(
              in_features=128, out_features=128, bias=True
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            )input_quant=True, weight_quant=True)
            (mlp): Mlp(
              (fc1): (QuantConv2d_ar(
                128, 256, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (act): ReLU()
              (fc2): (QuantConv2d_ar(
                256, 128, kernel_size=(1, 1), stride=(1, 1)
                (input_quantizer): UniformQuantizer_diff()
                (weight_quantizer): UniformQuantizer_diff()
              ), input_quant=True, weight_quant=True)
              (drop): Dropout(p=0.1, inplace=False)
            )
            (pos_enc): PositionEmbeddingSine()
            (concat_conv): (QuantConv2d_ar(
              256, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
            (gn): GroupNorm(8, 128, eps=1e-05, affine=True)
            (out_conv): (QuantConv2d_ar(
              128, 128, kernel_size=(1, 1), stride=(1, 1)
              (input_quantizer): UniformQuantizer_diff()
              (weight_quantizer): UniformQuantizer_diff()
            ), input_quant=True, weight_quant=True)
          )
        )
      )
    )
    (mid_block1): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=1024, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=True, weight_quant=True)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): Identity()
    )
    (mid_attn): Residual(
      (fn): PreNorm(
        (fn): Attention(
          (to_qkv): (QuantConv2d_ar(
            512, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
          (to_out): (QuantConv2d_ar(
            128, 512, kernel_size=(1, 1), stride=(1, 1)
            (input_quantizer): UniformQuantizer_diff()
            (weight_quantizer): UniformQuantizer_diff()
          ), input_quant=True, weight_quant=True)
        )
        (norm): LayerNorm()
      )
    )
    (mid_block2): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=1024, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=True, weight_quant=True)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 512, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): Identity()
    )
    (decouple1): MySequential(
      (0): GroupNorm(8, 512, eps=1e-05, affine=True)
      (1): (QuantConv2d_ar(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (2): BlockFFT()
    )
    (decouple2): MySequential(
      (0): GroupNorm(8, 512, eps=1e-05, affine=True)
      (1): (QuantConv2d_ar(
        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
      (2): BlockFFT()
    )
    (final_res_block): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=256, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=True, weight_quant=True)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
    )
    (final_conv): (QuantConv2d_ar(
      128, 3, kernel_size=(1, 1), stride=(1, 1)
      (input_quantizer): UniformQuantizer_diff()
      (weight_quantizer): UniformQuantizer_diff()
    ), input_quant=True, weight_quant=True)
    (final_res_block2): ResnetBlock(
      (mlp): MySequential(
        (0): SiLU()
        (1): (QuantLinear_diff(
          in_features=512, out_features=256, bias=True
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        )input_quant=True, weight_quant=True)
      )
      (block1): Block(
        (proj): (QuantConv2d_ar(
          256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (block2): Block(
        (proj): (QuantConv2d_ar(
          128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)
          (input_quantizer): UniformQuantizer_diff()
          (weight_quantizer): UniformQuantizer_diff()
        ), input_quant=True, weight_quant=True)
        (norm): GroupNorm(8, 128, eps=1e-05, affine=True)
        (act): SiLU()
      )
      (res_conv): (QuantConv2d_ar(
        256, 128, kernel_size=(1, 1), stride=(1, 1)
        (input_quantizer): UniformQuantizer_diff()
        (weight_quantizer): UniformQuantizer_diff()
      ), input_quant=True, weight_quant=True)
    )
    (final_conv2): (QuantConv2d_ar(
      128, 3, kernel_size=(1, 1), stride=(1, 1)
      (input_quantizer): UniformQuantizer_diff()
      (weight_quantizer): UniformQuantizer_diff()
    ), input_quant=True, weight_quant=True)
  )
  (first_stage_model): AutoencoderKL(
    (encoder): Encoder(
      (conv_in): Conv2d(1, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (down): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (downsample): Downsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
      )
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)
      (conv_out): Conv2d(512, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (decoder): Decoder(
      (conv_in): Conv2d(3, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (mid): Module(
        (block_1): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
        (attn_1): AttnBlock(
          (norm): GroupNorm(32, 512, eps=1e-06, affine=True)
          (q): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (k): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (v): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
          (proj_out): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
        )
        (block_2): ResnetBlock(
          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (up): ModuleList(
        (0): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)
              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
        )
        (1): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (nin_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)
              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
        (2): Module(
          (block): ModuleList(
            (0): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (1): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
            (2): ResnetBlock(
              (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)
              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
              (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)
              (dropout): Dropout(p=0.0, inplace=False)
              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            )
          )
          (attn): ModuleList()
          (upsample): Upsample(
            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          )
        )
      )
      (norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)
      (conv_out): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (conv_logvar): Conv2d(128, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (loss): LPIPSWithDiscriminator(
      (perceptual_loss): LPIPS(
        (scaling_layer): ScalingLayer()
        (net): vgg16(
          (slice1): Sequential(
            (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (1): ReLU(inplace=True)
            (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (3): ReLU(inplace=True)
          )
          (slice2): Sequential(
            (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (6): ReLU(inplace=True)
            (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (8): ReLU(inplace=True)
          )
          (slice3): Sequential(
            (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (11): ReLU(inplace=True)
            (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (13): ReLU(inplace=True)
            (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (15): ReLU(inplace=True)
          )
          (slice4): Sequential(
            (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (18): ReLU(inplace=True)
            (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (20): ReLU(inplace=True)
            (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (22): ReLU(inplace=True)
          )
          (slice5): Sequential(
            (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
            (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (25): ReLU(inplace=True)
            (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (27): ReLU(inplace=True)
            (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (29): ReLU(inplace=True)
          )
        )
        (lin0): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin1): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(128, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin2): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin3): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (lin4): NetLinLayer(
          (model): Sequential(
            (0): Dropout(p=0.5, inplace=False)
            (1): Conv2d(512, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
      (discriminator): NLayerDiscriminator(
        (main): Sequential(
          (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
          (1): LeakyReLU(negative_slope=0.2, inplace=True)
          (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (4): LeakyReLU(negative_slope=0.2, inplace=True)
          (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)
          (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (7): LeakyReLU(negative_slope=0.2, inplace=True)
          (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)
          (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (10): LeakyReLU(negative_slope=0.2, inplace=True)
          (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (quant_conv): Conv2d(6, 6, kernel_size=(1, 1), stride=(1, 1))
    (post_quant_conv): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1))
  )
)
